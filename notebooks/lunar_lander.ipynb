{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-39y19imy because the default path (/home/guest/d11922022/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/guest/d11922022/github/openai-gym')\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "# https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW12/HW12_EN.pdf\n",
    "# https://github.com/john-hu/rl\n",
    "# https://medium.com/no-sliver-bullet/%E5%BC%B7%E5%8C%96%E5%AD%B8%E7%BF%92-reinforcement-learning-lunar-lander-v2-1291d48b71c3\n",
    "# https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/tree/master/preTrained\n",
    "\"\"\"\n",
    "Rewards\n",
    "Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 \n",
    "points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it \n",
    "receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each \n",
    "leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the \n",
    "side engine is -0.03 points each frame. Solved is 200 points.\n",
    "\"\"\"\n",
    "\n",
    "from utils import plot_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33md11922022\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guest/d11922022/github/openai-gym/wandb/run-20240404_141533-yjg6rspq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://140.112.31.158:8080/d11922022/lunalander/runs/yjg6rspq/workspace' target=\"_blank\">usual-forest-16</a></strong> to <a href='http://140.112.31.158:8080/d11922022/lunalander' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://140.112.31.158:8080/d11922022/lunalander' target=\"_blank\">http://140.112.31.158:8080/d11922022/lunalander</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://140.112.31.158:8080/d11922022/lunalander/runs/yjg6rspq/workspace' target=\"_blank\">http://140.112.31.158:8080/d11922022/lunalander/runs/yjg6rspq/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af6d9f15b5d448d82331eda0e9b29db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_total_rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m luna_lander_trainer \u001b[38;5;241m=\u001b[39m trainer(agent, training_config, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m avg_total_reward, avg_final_reward, avg_episode_steps \u001b[38;5;241m=\u001b[39m luna_lander_trainer\u001b[38;5;241m.\u001b[39mtrain(discounted_future_reward)\n\u001b[0;32m---> 36\u001b[0m plot_rewards(\u001b[43mavg_total_rewards\u001b[49m, avg_final_rewards)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_total_rewards' is not defined"
     ]
    }
   ],
   "source": [
    "from scripts.train import trainer\n",
    "from agents import (\n",
    "    PolicyGradientNetwork,\n",
    "    PolicyGradientAgent\n",
    ") \n",
    "\n",
    "input_size, hidden_sizes, output_size = 8, [10, 10], 4\n",
    "network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "training_config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"discounted_future_reward\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"gamma\": 0.99,\n",
    "    \"batch\": 500,\n",
    "    \"episode\": 5,\n",
    "    \"model_path\": \"\",\n",
    "    \"model_name\": \"\",\n",
    "    \"greedy\": False,\n",
    "    \"info\": \"network 8x10x10x4\"\n",
    "}\n",
    "\n",
    "def discounted_future_reward(seq_rewards, rewards, gamma):\n",
    "    n = len(seq_rewards)\n",
    "\n",
    "    for i in range(2, n+1):\n",
    "        seq_rewards[-i] += gamma * (seq_rewards[-i+1])\n",
    "    rewards[-len(seq_rewards):] = seq_rewards\n",
    "    return rewards\n",
    "\n",
    "luna_lander_trainer = trainer(agent, training_config, seed=1)\n",
    "avg_total_reward, avg_final_reward, avg_episode_steps = luna_lander_trainer.train(discounted_future_reward)\n",
    "plot_rewards(avg_total_rewards, avg_final_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luna_lander_trainer.config['greedy'] = False\n",
    "total_rewards, final_rewards, episode_steps = luna_lander_trainer.test(discounted_future_reward)\n",
    "plot_rewards(total_rewards, final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考\n",
    "# https://github.com/gabrielgarza/openai-gym-policy-gradient/blob/master/policy_gradient.py\n",
    "# https://github.com/sudharsan13296/Hands-On-Reinforcement-Learning-With-Python/blob/master/11.%20Policy%20Gradients%20and%20Optimization/11.2%20Lunar%20Lander%20Using%20Policy%20Gradients.ipynb\n",
    "# https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%204/4.3%20Policy%20Gradients%20REINFORCE%20Baseline.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 543\n",
    "\n",
    "def fix(env, seed):\n",
    "    \"\"\"\n",
    "    Fix random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        env: Environment object.\n",
    "        seed (int): Seed value for random number generation.\n",
    "\n",
    "    Notes:\n",
    "        This function fixes the random seed for various libraries and modules\n",
    "        to ensure reproducibility of results in environments with randomness.\n",
    "\n",
    "    Deprecated:\n",
    "        - env.seed(seed): Deprecated method to set seed for environment.\n",
    "        - torch.set_deterministic(True): Deprecated method to set PyTorch \n",
    "          deterministic mode.\n",
    "\n",
    "    \"\"\"\n",
    "    # -deprecated- env.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed = seed\n",
    "    # -deprecated- torch.set_deterministic(True)\n",
    "    torch.use_deterministic_algorithms = True\n",
    "    torch.are_deterministic_algorithms_enabled = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example():\n",
    "    \"\"\"\n",
    "    Run an example of LunarLander-v2 environment.\n",
    "\n",
    "    Notes:\n",
    "        This function demonstrates running an example of the LunarLander-v2\n",
    "        environment using Gym. It fixes the random seed for reproducibility,\n",
    "        renders the environment, and displays the frames.\n",
    "\n",
    "    \"\"\"\n",
    "    env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "    fix(env, SEED)\n",
    "    env.reset()\n",
    "    img = plt.imshow(env.render()) # only call this once\n",
    "    total_reward = 0\n",
    "    for _ in range(100):\n",
    "        img.set_data(env.render()) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(f'\\r{reward:.4f}, total_reward:{total_reward:.4f}, terminated:{terminated}')\n",
    "\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.observation_space\n",
    "# env.action_space\n",
    "# initial_state, initial_info = env.reset(seed=seed)\n",
    "# random_action = env.action_space.sample()\n",
    "# observation, reward, terminated, truncated, info = env.step(random_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gymnasium as gym\n",
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "# trigger = lambda t: t % 10 == 0\n",
    "# env = RecordVideo(env, video_folder=\"./video/video1\", episode_trigger=trigger, disable_logger=True)\n",
    "# for i in range(50):\n",
    "#     termination, truncation = False, False\n",
    "#     _ = env.reset(seed=123)\n",
    "#     while not (termination or truncation):\n",
    "#         obs, rew, termination, truncation, info = env.step(env.action_space.sample())\n",
    "\n",
    "# env.close()\n",
    "# len(os.listdir(\"./video/video1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_run(env, agent, seed=SEED, num_episode=1, record_video=False, video_folder = './video/tmp', greedy=False):\n",
    "    agent.network.eval()\n",
    "    \n",
    "    if not record_video:\n",
    "        img = plt.imshow(env.render())\n",
    "    \n",
    "    for episode in range(num_episode):\n",
    "        if not record_video:\n",
    "            env_ = env\n",
    "        else:\n",
    "            trigger = lambda t: t%10 == 0\n",
    "            action_mode = 'greedy' if greedy else 'sample'\n",
    "            env_ = RecordVideo(env, \n",
    "                               video_folder=video_folder+f'-{action_mode}-ep{episode}', \n",
    "                               episode_trigger=trigger)\n",
    "            \n",
    "        state, info = env_.reset(seed=seed)\n",
    "        terminated, truncated = False, False\n",
    "        total_reward = 0\n",
    "        while not (terminated or truncated):\n",
    "            action, log_prob = agent.sample(state, greedy)\n",
    "            next_state, reward, terminated, truncated, info = env_.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            print(f'reward: {reward}, total_reward:{total_reward}, terminated:{terminated}, truncated:{truncated}')\n",
    "\n",
    "            if not record_video:\n",
    "                img.set_data(env.render())\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "        print(f'reward: {reward}, total_reward:{total_reward}, terminated:{terminated}, truncated:{truncated}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.relu(self.fc1(state))\n",
    "        hid = torch.relu(self.fc2(hid))\n",
    "        return F.softmax(self.fc3(hid), dim=-1)\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyGradientNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        # Add input layer\n",
    "        self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        # Add output layer\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Forward pass through each hidden layer\n",
    "        for layer in self.hidden_layers:\n",
    "            state = F.relu(layer(state))\n",
    "        \n",
    "        # Output layer\n",
    "        return F.softmax(self.output_layer(state), dim=-1)\n",
    "\n",
    "# Example usage:\n",
    "input_size = 8\n",
    "hidden_sizes = [10, 10]  # You can adjust this list to add more hidden layers or change sizes\n",
    "output_size = 4\n",
    "\n",
    "# Initialize the network\n",
    "network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# # Example forward pass\n",
    "# state = torch.randn(1, input_size)  # Example input\n",
    "# output = model(state)\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16,16)\n",
    "\n",
    "        # Actor network\n",
    "        self.actor_fc3 = nn.Linear(16, 4)\n",
    "\n",
    "        # Critic network\n",
    "        self.critic_fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.relu(self.fc1(state))\n",
    "        hid = torch.relu(self.fc2(hid))\n",
    "\n",
    "        # Actor branch\n",
    "        actor_hid = torch.relu(hid)\n",
    "        actor_out = F.softmax(self.actor_fc3(actor_hid), dim=-1)\n",
    "\n",
    "        # Critic branch\n",
    "        critic_hid = torch.relu(hid)\n",
    "        critic_out = self.critic_fc3(critic_hid)\n",
    "\n",
    "        return actor_out, critic_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
    "\n",
    "* `learn()`：update the policy network from log probabilities and rewards.\n",
    "* `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent():\n",
    "    def __init__(self, network, lr=1e-3, epochs=500):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=1e-3)\n",
    "        # self.lr_scheduler = CosineAnnealingLR(self.optimizer, self.epochs, eta_min=1e-6)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # self.lr_scheduler.step()\n",
    "        \n",
    "    def sample(self, state, greedy=False):\n",
    "        action_prob = self.network(torch.FloatTensor(np.array(state)))\n",
    "        action_dist = Categorical(action_prob)\n",
    "        if not greedy:\n",
    "            action = action_dist.sample()\n",
    "        else:\n",
    "            action = action_prob.argmax()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def save(self, PATH): # You should not revise this\n",
    "        Agent_Dict = {\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(Agent_Dict, PATH)\n",
    "\n",
    "    def load(self, PATH): # You should not revise this\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.network.load_state_dict(checkpoint[\"network\"])\n",
    "        #如果要儲存過程或是中斷訓練後想繼續可以用喔\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent():\n",
    "    def __init__(self, network, lr=1e-3, epochs=500):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.network = network\n",
    "        self.optimizer_actor = optim.SGD(self.network.parameters(), lr=lr)\n",
    "        self.optimizer_critic = optim.SGD(self.network.parameters(), lr=lr)  # Use the same learning rate for simplicity\n",
    "        self.lr_scheduler_actor = CosineAnnealingLR(self.optimizer_actor, self.epochs, eta_min=1e-6)\n",
    "        self.lr_scheduler_critic = CosineAnnealingLR(self.optimizer_critic, self.epochs, eta_min=1e-6)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "    def actor_learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.lr_scheduler_actor.step()\n",
    "\n",
    "    def critic_learn(self, values, rewards):\n",
    "        loss = F.mse_loss(values, rewards)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.lr_scheduler_critic.step()\n",
    "\n",
    "    def sample(self, state, greedy=False):\n",
    "        action_prob, value = self.network(torch.FloatTensor(np.array(state)))\n",
    "        \n",
    "        # actor branch\n",
    "        action_dist = Categorical(action_prob)\n",
    "        if not greedy:\n",
    "            action = action_dist.sample()\n",
    "        else:\n",
    "            action = action_prob.argmax()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "\n",
    "        return action.item(), log_prob, value\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build a network and agent to start training\n",
    "# network = PolicyGradientNetwork()\n",
    "# agent = PolicyGradientAgent(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import random\n",
    "\n",
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"lunalander\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"Agent\": \"PolicyGradientAgent\",\n",
    "#     \"reward\": \"normalized(step_reward)*step_log_prob\",\n",
    "#     \"learning_rate\": 1e-3,\n",
    "#     \"epochs\": 500,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "input_size = 8\n",
    "hidden_sizes = [10, 10]  # You can adjust this list to add more hidden layers or change sizes\n",
    "output_size = 4\n",
    "\n",
    "# Initialize the network\n",
    "# network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "\n",
    "from utils import fix\n",
    "# network = PolicyGradientNetwork()\n",
    "network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "agent = PolicyGradientAgent(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fix\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "# network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "# agent = PolicyGradientAgent(network)\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "fix(env, 1)\n",
    "\n",
    "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
    "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
    "NUM_BATCH =500        # 總共更新 500 次\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "gamma = 0.992\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # 蒐集訓練資料\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        total_reward, total_step = 0, 0\n",
    "        seq_rewards = []\n",
    "        while True:\n",
    "\n",
    "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            seq_rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            rewards.append(reward) #改這裡\n",
    "            # ! 重要 ！\n",
    "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
    "            #                                                       reward :     r1, r2 ,r3 ......\n",
    "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
    "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement DQN\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                \n",
    "                n = len(seq_rewards)\n",
    "                # for i in range(n-1, -1, -1):\n",
    "                #     rewards[i-n] = sum([r*gamma**j for j, r in enumerate(seq_rewards[i:], 0)])\n",
    "\n",
    "                for i in range(2, n+1):\n",
    "                    seq_rewards[-i] += gamma * (seq_rewards[-i+1])\n",
    "                rewards[-len(seq_rewards):] = seq_rewards\n",
    "                break\n",
    "        agent.save(f'./models/policy_gradient@{episode}.pt')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # print(f\"rewards looks like \", np.shape(rewards))  \n",
    "    # print(f\"log_probs looks like \", np.shape([_.item() for _ in log_probs]))    \n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    # rewards = np.concatenate(rewards, axis=0)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy( np.array(rewards) ).size())\n",
    "\n",
    "plot_rewards(total_rewards)\n",
    "plot_rewards(avg_total_rewards, avg_final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_run(env, agent, seed=None, num_episode=5, record_video=True, video_folder = './video/tmp', greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_run(env, agent, seed=None, num_episode=5, record_video=True, video_folder = './video/tmp', greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "from utils import fix\n",
    "\n",
    "input_size = 8\n",
    "hidden_sizes = [10, 10]  # You can adjust this list to add more hidden layers or change sizes\n",
    "output_size = 4\n",
    "network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "# network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "# agent = PolicyGradientAgent(network)\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "fix(env, 1)\n",
    "\n",
    "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
    "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
    "NUM_BATCH =500        # 總共更新 500 次\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "gamma = 0.992\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # 蒐集訓練資料\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        total_reward, total_step = 0, 0\n",
    "        seq_rewards = []\n",
    "        while True:\n",
    "\n",
    "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            seq_rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            rewards.append(reward) #改這裡\n",
    "            # ! 重要 ！\n",
    "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
    "            #                                                       reward :     r1, r2 ,r3 ......\n",
    "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
    "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement DQN\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                \n",
    "                n = len(seq_rewards)\n",
    "                # for i in range(n-1, -1, -1):\n",
    "                #     rewards[i-n] = sum([r*gamma**j for j, r in enumerate(seq_rewards[i:], 0)])\n",
    "\n",
    "                for i in range(2, n+1):\n",
    "                    seq_rewards[-i] += gamma * (seq_rewards[-i+1])\n",
    "                rewards[-len(seq_rewards):] = seq_rewards\n",
    "                break\n",
    "        agent.save(f'./models/policy_gradient2@{episode}.pt')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # print(f\"rewards looks like \", np.shape(rewards))  \n",
    "    # print(f\"log_probs looks like \", np.shape([_.item() for _ in log_probs]))    \n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    # rewards = np.concatenate(rewards, axis=0)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy( np.array(rewards) ).size())\n",
    "\n",
    "plot_rewards(total_rewards)\n",
    "plot_rewards(avg_total_rewards, avg_final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_run(env, agent, seed=None, num_episode=5, record_video=True, video_folder = './video/tmp2', greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "\n",
    "config = {\"reward\":\"MC\"}\n",
    "\n",
    "agent.network.train()  # Switch network into training mode \n",
    "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
    "NUM_BATCH = 10        # totally update the agent for 400 time\n",
    "gamma = 0.9\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "\n",
    "NUM_BATCH=1\n",
    "EPISODE_PER_BATCH = 1\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "\n",
    "    \n",
    "for batch in range(NUM_BATCH):#prg_bar:\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "    total_steps = []\n",
    "    \n",
    "    # collect trajectory\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        img = plt.imshow(env.render())\n",
    "        total_reward, total_step = 0, 0\n",
    "        terminated, truncated = False, False\n",
    "        while not(terminated or truncated):\n",
    "        # while not (terminated or truncated):\n",
    "            print(f'\\r{reward:.4f}, total_reward:{total_reward:.4f}, terminated:{terminated}')\n",
    "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # render\n",
    "            img.set_data(env.render()) # just update the data\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)     \n",
    "            # render       \n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            match config[\"reward\"]:\n",
    "                case \"normalized_step_reward\":\n",
    "                    rewards.append(reward) # change here\n",
    "                case \"MC\":\n",
    "                    rewards.append(reward*(gamma**total_step))\n",
    "    \n",
    "            total_step += 1\n",
    "\n",
    "            # ! IMPORTANT !\n",
    "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
    "            #                                                         rewards :     r1, r2 ,r3 ......\n",
    "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
    "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement Actor-Critic\n",
    "\n",
    "            # if terminated or truncated:\n",
    "            #    final_rewards.append(reward)\n",
    "            #    total_rewards.append(total_reward)\n",
    "            #    break\n",
    "\n",
    "        else:\n",
    "            final_rewards.append(reward)\n",
    "            total_rewards.append(total_reward)\n",
    "            total_steps.append(total_step)\n",
    "            state, info = env.reset()\n",
    "            #img.set_data(env.render()) # just update the data\n",
    "            #display.display(plt.gcf())\n",
    "            #display.clear_output(wait=True)     \n",
    "        \n",
    "    \n",
    "    # update agent\n",
    "    mean_rewards = np.mean(rewards)\n",
    "    std_rewards = np.std(rewards)\n",
    "\n",
    "    match config[\"reward\"]:\n",
    "        case \"normalized_step_reward\":\n",
    "            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "        case \"MC\":\n",
    "            rewards_ = []\n",
    "            start = [sum(total_steps[:i]) for i in range(len(total_steps))]\n",
    "            end =  [sum(total_steps[:i]) for i in range(1, len(total_steps)+1)]\n",
    "\n",
    "            for s, e in zip(start, end):\n",
    "                rewards_ = rewards_ + [ sum(rewards[s:e][i:])/(gamma**i) for i in range(len(rewards[s:e])) ]\n",
    "            \n",
    "            #rewards = rewards_\n",
    "            #rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)\n",
    "    \n",
    "\n",
    "    #print(f\"rewards looks like \", np.shape(rewards))  \n",
    "    #print(f\"log_probs looks like \", np.shape(log_probs))     \n",
    "    # record training process\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # update agent\n",
    "    # rewards = np.concatenate(rewards, axis=0)\n",
    "    mean_rewards = np.mean(rewards)\n",
    "    std_rewards = np.std(rewards)\n",
    "    # rewards = (np.array(rewards) - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "    #rewards = np.array(np.sum(rewards))\n",
    "    #rewards = np.array([sum(rewards[i:])/(gamma**i) for i in range(len(rewards))])\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(np.array(rewards_)))\n",
    "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy( np.array(rewards_) ).size())\n",
    "    \n",
    "    # wandb.log({\"mean rewards\":mean_rewards, \n",
    "    #            \"std reward\":std_rewards, \n",
    "    #            \"avg_total_reward\": avg_total_reward, \n",
    "    #            \"avg_final_reward\": avg_final_reward, \n",
    "    #            \"avg_episode_steps\":len(rewards)/EPISODE_PER_BATCH}\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
    "NUM_BATCH = 500        # totally update the agent for 400 time\n",
    "\n",
    "def agent_run(\n",
    "    env, \n",
    "    agent, \n",
    "    config, \n",
    "    episode_per_batch=EPISODE_PER_BATCH, \n",
    "    num_batch=NUM_BATCH, \n",
    "    gamma=0.99,\n",
    "    step_punish = -0.15,\n",
    "    wandb_log = True,\n",
    "    verbose = False\n",
    "):\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project = config['project'], \n",
    "            # track hyperparameters and run metadata\n",
    "            config = config\n",
    "        )\n",
    "\n",
    "    match config['phase']:\n",
    "        case 'train':\n",
    "            agent.network.train()  # Switch network into training mode \n",
    "        case 'eval':\n",
    "            agent.network.eval()  # Switch network into eval mode \n",
    "    \n",
    "    num_batch, episode_per_batch = config['num_batch'], config['episode_per_batch']\n",
    "    avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "    prg_bar = tqdm(range(num_batch))\n",
    "    for batch in prg_bar:\n",
    "\n",
    "        log_probs, rewards = [], []\n",
    "        total_rewards, final_rewards = [], []\n",
    "        total_steps = []\n",
    "        # collect trajectory\n",
    "        for episode in range(episode_per_batch):\n",
    "            \n",
    "            state, info = env.reset()\n",
    "            total_reward, total_step = 0, 0\n",
    "            terminated, truncated = False, False\n",
    "            \n",
    "            while not(terminated or truncated):\n",
    "            # while not (terminated or truncated):\n",
    "                action, log_prob = agent.sample(state) # at, log(at|st)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                reward += step_punish\n",
    "                # if reward == -100:\n",
    "                #     reward = -10\n",
    "                \n",
    "\n",
    "                log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                match config[\"reward\"]:\n",
    "                    case \"normalized_step_reward\":\n",
    "                        rewards.append(reward) # change here\n",
    "                    case \"MC\":\n",
    "                        rewards.append(reward*(gamma**total_step))\n",
    "                \n",
    "                total_step += 1\n",
    "                # ! IMPORTANT !\n",
    "                # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
    "                #                                                         rewards :     r1, r2 ,r3 ......\n",
    "                # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
    "                #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
    "                # boss : implement Actor-Critic\n",
    "\n",
    "                # if terminated or truncated:\n",
    "                #    final_rewards.append(reward)\n",
    "                #    total_rewards.append(total_reward)\n",
    "                #    break\n",
    "\n",
    "            else:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                total_steps.append(total_step)\n",
    "                \n",
    "            \n",
    "\n",
    "        # record training process\n",
    "        avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "        avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "        avg_total_rewards.append(avg_total_reward)\n",
    "        avg_final_rewards.append(avg_final_reward)\n",
    "        avg_episode_steps = sum(total_steps)/len(total_steps)\n",
    "        prg_bar.set_description(f\"\\r Total reward: {avg_total_reward: 4.1f}, Final reward: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "        # update agent\n",
    "        mean_rewards = np.mean(rewards)\n",
    "        std_rewards = np.std(rewards)\n",
    "\n",
    "        match config[\"reward\"]:\n",
    "            case \"normalized_step_reward\":\n",
    "                rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "            case \"MC\":\n",
    "                rewards_ = []\n",
    "                start = [sum(total_steps[:i]) for i in range(len(total_steps))]\n",
    "                end =  [sum(total_steps[:i]) for i in range(1, len(total_steps)+1)]\n",
    "\n",
    "                for s, e in zip(start, end):\n",
    "                    rewards_ = rewards_ + [ sum(rewards[s:e][i:])/(gamma**i) for i in range(len(rewards[s:e])) ]\n",
    "                \n",
    "                rewards = rewards_\n",
    "                rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)\n",
    "        \n",
    "                #rewards = np.array([sum(rewards[i:])/(gamma**i) for i in range(len(rewards))])\n",
    "                #rewards = np.array(np.sum(rewards))\n",
    "\n",
    "        agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "        if verbose:\n",
    "            print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "            print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\"mean rewards\":mean_rewards, \n",
    "                    \"std reward\":std_rewards, \n",
    "                    \"avg_total_reward\": avg_total_reward, \n",
    "                    \"avg_final_reward\": avg_final_reward, \n",
    "                    \"avg_episode_steps\":avg_episode_steps}\n",
    "            )\n",
    "\n",
    "    match config['phase']:\n",
    "        case 'train':\n",
    "            torch.save(agent.network.state_dict(), config['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
    "NUM_BATCH = 1        # totally update the agent for 400 time\n",
    "\n",
    "# def agent_run(\n",
    "#     env, \n",
    "#     agent, \n",
    "#     config, \n",
    "#     episode_per_batch=EPISODE_PER_BATCH, \n",
    "#     num_batch=NUM_BATCH, \n",
    "#     gamma=0.99,\n",
    "#     wandb_log = True,\n",
    "#     verbose = False\n",
    "# ):\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 1,\n",
    "    \"episode_per_batch\": 5,\n",
    "    \"output_dir\": \"./models/policy_gradient_normalized_step_reward\"\n",
    "}\n",
    "episode_per_batch=EPISODE_PER_BATCH \n",
    "num_batch=NUM_BATCH\n",
    "gamma=0.99\n",
    "wandb_log = True\n",
    "verbose = False\n",
    "\n",
    "if True:\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project = config['project'], \n",
    "            # track hyperparameters and run metadata\n",
    "            config = config\n",
    "        )\n",
    "\n",
    "    match config['phase']:\n",
    "        case 'train':\n",
    "            agent.network.train()  # Switch network into training mode \n",
    "        case 'eval':\n",
    "            agent.network.eval()  # Switch network into eval mode \n",
    "    \n",
    "    num_batch, episode_per_batch = config['num_batch'], config['episode_per_batch']\n",
    "    avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "    prg_bar = tqdm(range(num_batch))\n",
    "    for batch in prg_bar:\n",
    "\n",
    "        log_probs, rewards = [], []\n",
    "        total_rewards, final_rewards = [], []\n",
    "        total_steps = []\n",
    "        original_rewards = []\n",
    "        # collect trajectory\n",
    "        for episode in range(episode_per_batch):\n",
    "            \n",
    "            state, info = env.reset()\n",
    "            total_reward, total_step = 0, 0\n",
    "            terminated, truncated = False, False\n",
    "            \n",
    "            while not(terminated or truncated):\n",
    "            # while not (terminated or truncated):\n",
    "                action, log_prob = agent.sample(state) # at, log(at|st)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                \n",
    "\n",
    "                log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                match config[\"reward\"]:\n",
    "                    case \"normalized_step_reward\":\n",
    "                        rewards.append(reward) # change here\n",
    "                    case \"MC\":\n",
    "                        rewards.append(reward*(gamma**total_step))\n",
    "                        original_rewards.append(reward)\n",
    "                \n",
    "                total_step += 1\n",
    "                # ! IMPORTANT !\n",
    "                # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
    "                #                                                         rewards :     r1, r2 ,r3 ......\n",
    "                # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
    "                #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
    "                # boss : implement Actor-Critic\n",
    "\n",
    "                # if terminated or truncated:\n",
    "                #    final_rewards.append(reward)\n",
    "                #    total_rewards.append(total_reward)\n",
    "                #    break\n",
    "\n",
    "            else:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                total_steps.append(total_step)\n",
    "                \n",
    "            \n",
    "\n",
    "        # record training process\n",
    "        avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "        avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "        avg_total_rewards.append(avg_total_reward)\n",
    "        avg_final_rewards.append(avg_final_reward)\n",
    "        avg_episode_steps = sum(total_steps)/len(total_steps)\n",
    "        prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "        # update agent\n",
    "        mean_rewards = np.mean(rewards)\n",
    "        std_rewards = np.std(rewards)\n",
    "\n",
    "        match config[\"reward\"]:\n",
    "            case \"normalized_step_reward\":\n",
    "                rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "            case \"MC\":\n",
    "                rewards_ = []\n",
    "                start = [sum(total_steps[:i]) for i in range(len(total_steps))]\n",
    "                end =  [sum(total_steps[:i]) for i in range(1, len(total_steps)+1)]\n",
    "\n",
    "                for s, e in zip(start, end):\n",
    "                    rewards_ = rewards_ + [ sum(rewards[s:e][i:])/(gamma**i) for i in range(len(rewards[s:e])) ]\n",
    "                \n",
    "                rewards = np.array(rewards_)\n",
    "                #rewards = rewards_\n",
    "                #rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)\n",
    "\n",
    "\n",
    "        agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "        if verbose:\n",
    "            print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "            print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\"mean rewards\":mean_rewards, \n",
    "                    \"std reward\":std_rewards, \n",
    "                    \"avg_total_reward\": avg_total_reward, \n",
    "                    \"avg_final_reward\": avg_final_reward, \n",
    "                    \"avg_episode_steps\":avg_episode_steps}\n",
    "            )\n",
    "\n",
    "    match config['phase']:\n",
    "        case 'train':\n",
    "            torch.save(agent.network.state_dict(), config['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"normalized_step_reward\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 500,\n",
    "    \"episode_per_batch\": 5,\n",
    "    \"output_dir\": \"./models/policy_gradient_normalized_step_reward\"\n",
    "}\n",
    "\n",
    "agent_run(env, agent, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "agent.network.load_state_dict(torch.load(config[\"output_dir\"]))\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_normalized_step_reward\", greedy=True)\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_normalized_step_reward\", greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 500,\n",
    "    \"episode_per_batch\": 5,\n",
    "    \"output_dir\": \"./models/policy_gradient_MC_gamma_0.7\"\n",
    "}\n",
    "\n",
    "agent_run(env, agent, config=config, gamma=0.7)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "agent.network.load_state_dict(torch.load(config[\"output_dir\"]))\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_0.7_advantage_step_punish\", greedy=True)\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_0.7_advantage_step_punish\", greedy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 500,\n",
    "    \"episode_per_batch\": 1,\n",
    "    \"output_dir\": \"./models/policy_gradient_MC_gamma_0.90\"\n",
    "}\n",
    "\n",
    "agent_run(env, agent, config=config, gamma=0.90)\n",
    "\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "agent.network.load_state_dict(torch.load(config[\"output_dir\"]))\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_0.90\", greedy=True)\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_0.90\", greedy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 500,\n",
    "    \"episode_per_batch\": 5,\n",
    "    \"output_dir\": \"./models/policy_gradient_MC_gamma_0.90_episode_per_batch_5_normalized\"\n",
    "}\n",
    "\n",
    "agent_run(env, agent, config=config, gamma=0.90)\n",
    "\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "agent.network.load_state_dict(torch.load(config[\"output_dir\"]))\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_090_episode_per_batch_5_normalized\", greedy=True)\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_090_episode_per_batch_5_normalized\", greedy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 10000,\n",
    "    \"episode_per_batch\": 1,\n",
    "    \"output_dir\": \"./models/policy_gradient_MC_gamma_0.90\"\n",
    "}\n",
    "\n",
    "agent_run(env, agent, config=config, gamma=0.8)\n",
    "\n",
    "\n",
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "agent.network.load_state_dict(torch.load(config[\"output_dir\"]))\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_0.90_batch_10000\", greedy=True)\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/policy_gradient_MC_gamma_0.90_batch_10000\", greedy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_run(env, agent, seed=seed, num_episode=1, record_video=False, video_folder = './video/tmp', greedy=False):\n",
    "    agent.network.eval()\n",
    "    \n",
    "    if not record_video:\n",
    "        img = plt.imshow(env.render())\n",
    "    \n",
    "    for episode in range(num_episode):\n",
    "        if not record_video:\n",
    "            env_ = env\n",
    "        else:\n",
    "            trigger = lambda t: t%10 == 0\n",
    "            action_mode = 'greedy' if greedy else 'sample'\n",
    "            env_ = RecordVideo(env, video_folder=video_folder+f'-{action_mode}-{episode}', episode_trigger=trigger)\n",
    "            \n",
    "        state, info = env_.reset(seed=seed)\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            action, log_prob, value = agent.sample(state, greedy)\n",
    "            next_state, reward, terminated, truncated, info = env_.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            if not record_video:\n",
    "                img.set_data(env.render())\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCH=500\n",
    "EPISODE_PER_BATCH = 5\n",
    "\n",
    "network = ActorCriticNetwork()\n",
    "agent = ActorCriticAgent(network, epochs=NUM_BATCH//2)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"ActorCriticAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 500,\n",
    "    \"episode_per_batch\": 5,\n",
    "    \"output_dir\": \"./models/policy_gradient_actor_critcs\"\n",
    "}\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project = 'lunalander', \n",
    "    # track hyperparameters and run metadata\n",
    "    config = config\n",
    ")\n",
    "\n",
    "config = {\"reward\":\"MC\"}\n",
    "\n",
    "agent.network.train()  # Switch network into training mode \n",
    "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
    "NUM_BATCH = 500       # totally update the agent for 400 time\n",
    "gamma = 0.9\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "\n",
    "    \n",
    "for b, batch in enumerate(prg_bar):\n",
    "    log_probs, rewards, values = [], [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "    total_steps = []\n",
    "    advantages = []\n",
    "    \n",
    "    # collect trajectory\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        #img = plt.imshow(env.render())\n",
    "        total_reward, total_step = 0, 0\n",
    "        terminated, truncated = False, False\n",
    "        while not(terminated or truncated):\n",
    "        # while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, value = agent.sample(state) # at, log(at|st)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, next_value = agent.sample(next_state)\n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            match config[\"reward\"]:\n",
    "                case \"normalized_step_reward\":\n",
    "                    rewards.append(reward) # change here\n",
    "                case \"MC\":\n",
    "                    rewards.append(reward*(gamma**total_step))\n",
    "                    values.append(value)\n",
    "                    advantages.append(reward + next_value - value)\n",
    "    \n",
    "            total_step += 1\n",
    "\n",
    "            # ! IMPORTANT !\n",
    "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
    "            #                                                         rewards :     r1, r2 ,r3 ......\n",
    "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
    "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement Actor-Critic\n",
    "\n",
    "            # if terminated or truncated:\n",
    "            #    final_rewards.append(reward)\n",
    "            #    total_rewards.append(total_reward)\n",
    "            #    break\n",
    "\n",
    "        else:\n",
    "            final_rewards.append(reward)\n",
    "            total_rewards.append(total_reward)\n",
    "            total_steps.append(total_step)\n",
    "            state, info = env.reset()\n",
    "            #img.set_data(env.render()) # just update the data\n",
    "            #display.display(plt.gcf())\n",
    "            #display.clear_output(wait=True)     \n",
    "        \n",
    "    \n",
    "    # update agent\n",
    "    mean_rewards = np.mean(rewards)\n",
    "    std_rewards = np.std(rewards)\n",
    "\n",
    "    match config[\"reward\"]:\n",
    "        case \"normalized_step_reward\":\n",
    "            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "        case \"MC\":\n",
    "            rewards_ = []\n",
    "            start = [sum(total_steps[:i]) for i in range(len(total_steps))]\n",
    "            end =  [sum(total_steps[:i]) for i in range(1, len(total_steps)+1)]\n",
    "\n",
    "            for s, e in zip(start, end):\n",
    "                rewards_ = rewards_ + [ sum(rewards[s:e][i:])/(gamma**i) for i in range(len(rewards[s:e])) ]\n",
    "            \n",
    "            #rewards = rewards_\n",
    "            #rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)\n",
    "    \n",
    "\n",
    "    #print(f\"rewards looks like \", np.shape(rewards))  \n",
    "    #print(f\"log_probs looks like \", np.shape(log_probs))     \n",
    "    # record training process\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # update agent\n",
    "    # rewards = np.concatenate(rewards, axis=0)\n",
    "    mean_rewards = np.mean(rewards)\n",
    "    std_rewards = np.std(rewards)\n",
    "    # rewards = (np.array(rewards) - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "    #rewards = np.array(np.sum(rewards))\n",
    "    #rewards = np.array([sum(rewards[i:])/(gamma**i) for i in range(len(rewards))])\n",
    "    if b % 2:\n",
    "        agent.critic_learn(torch.from_numpy(np.array(rewards_)).float(), torch.stack(values).squeeze())\n",
    "    else:\n",
    "        agent.actor_learn(torch.stack(log_probs), torch.stack(advantages).squeeze())\n",
    "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy( np.array(rewards_) ).size())\n",
    "    \n",
    "    wandb.log({\"mean rewards\":mean_rewards, \n",
    "               \"std reward\":std_rewards, \n",
    "               \"avg_total_reward\": avg_total_reward, \n",
    "               \"avg_final_reward\": avg_final_reward, \n",
    "               \"avg_episode_steps\":len(rewards)/EPISODE_PER_BATCH}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
    "NUM_BATCH = 500        # totally update the agent for 400 time\n",
    "\n",
    "def agent_run(\n",
    "    env, \n",
    "    agent, \n",
    "    config, \n",
    "    episode_per_batch=EPISODE_PER_BATCH, \n",
    "    num_batch=NUM_BATCH, \n",
    "    gamma=0.99,\n",
    "    step_punish = -0.15,\n",
    "    wandb_log = True,\n",
    "    verbose = False\n",
    "):\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project = config['project'], \n",
    "            # track hyperparameters and run metadata\n",
    "            config = config\n",
    "        )\n",
    "\n",
    "    # match config['phase']:\n",
    "    #     case 'train':\n",
    "    #         agent.network.train()  # Switch network into training mode \n",
    "    #     case 'eval':\n",
    "    #         agent.network.eval()  # Switch network into eval mode \n",
    "    \n",
    "    num_batch, episode_per_batch = config['num_batch'], config['episode_per_batch']\n",
    "    avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "    prg_bar = tqdm(range(num_batch))\n",
    "    for batch in prg_bar:\n",
    "\n",
    "        log_probs, rewards, values = [], [], []\n",
    "        total_rewards, final_rewards = [], []\n",
    "        total_steps = []\n",
    "        # collect trajectory\n",
    "        for episode in range(episode_per_batch):\n",
    "            \n",
    "            state, info = env.reset()\n",
    "            total_reward, total_step = 0, 0\n",
    "            terminated, truncated = False, False\n",
    "            \n",
    "            while not(terminated or truncated):\n",
    "            # while not (terminated or truncated):\n",
    "                \n",
    "                agent.network.train() \n",
    "                action, log_prob, value = agent.sample(state) # at, log(at|st)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                with torch.no_grad():\n",
    "                    _, _, next_value = agent.sample(next_state)\n",
    "                \n",
    "\n",
    "                # reward += step_punish\n",
    "                # if reward == -100:\n",
    "                #     reward = -10\n",
    "                \n",
    "\n",
    "                log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                match config[\"reward\"]:\n",
    "                    case \"normalized_step_reward\":\n",
    "                        rewards.append(reward) # change here\n",
    "                    case \"MC\":\n",
    "                        rewards.append(reward*(gamma**total_step))\n",
    "                        values.append(value)\n",
    "                        advantages.append(reward + next_value - value)\n",
    "                \n",
    "                total_step += 1\n",
    "                # ! IMPORTANT !\n",
    "                # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
    "                #                                                         rewards :     r1, r2 ,r3 ......\n",
    "                # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
    "                #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
    "                # boss : implement Actor-Critic\n",
    "\n",
    "                # if terminated or truncated:\n",
    "                #    final_rewards.append(reward)\n",
    "                #    total_rewards.append(total_reward)\n",
    "                #    break\n",
    "\n",
    "            else:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                total_steps.append(total_step)\n",
    "                \n",
    "            \n",
    "\n",
    "        # record training process\n",
    "        avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "        avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "        avg_total_rewards.append(avg_total_reward)\n",
    "        avg_final_rewards.append(avg_final_reward)\n",
    "        avg_episode_steps = sum(total_steps)/len(total_steps)\n",
    "        prg_bar.set_description(f\"\\r Total reward: {avg_total_reward: 4.1f}, Final reward: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "        # update agent\n",
    "        mean_rewards = np.mean(rewards)\n",
    "        std_rewards = np.std(rewards)\n",
    "\n",
    "        match config[\"reward\"]:\n",
    "            case \"normalized_step_reward\":\n",
    "                rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward \n",
    "            case \"MC\":\n",
    "                rewards_ = []\n",
    "                start = [sum(total_steps[:i]) for i in range(len(total_steps))]\n",
    "                end =  [sum(total_steps[:i]) for i in range(1, len(total_steps)+1)]\n",
    "\n",
    "                for s, e in zip(start, end):\n",
    "                    rewards_ = rewards_ + [ sum(rewards[s:e][i:])/(gamma**i) for i in range(len(rewards[s:e])) ]\n",
    "                \n",
    "                #rewards = rewards_\n",
    "                #rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)\n",
    "        \n",
    "                #rewards = np.array([sum(rewards[i:])/(gamma**i) for i in range(len(rewards))])\n",
    "                #rewards = np.array(np.sum(rewards))\n",
    "\n",
    "        #agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "        if b % 2:\n",
    "            agent.critic_learn(torch.from_numpy(np.array(rewards_)).float(), torch.stack(values).squeeze())\n",
    "        else:\n",
    "            agent.actor_learn(torch.stack(log_probs), torch.stack(advantages).squeeze())\n",
    "\n",
    "        if verbose:\n",
    "            print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "            print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\"mean rewards\":mean_rewards, \n",
    "                    \"std reward\":std_rewards, \n",
    "                    \"avg_total_reward\": avg_total_reward, \n",
    "                    \"avg_final_reward\": avg_final_reward, \n",
    "                    \"avg_episode_steps\":avg_episode_steps}\n",
    "            )\n",
    "\n",
    "    match config['phase']:\n",
    "        case 'train':\n",
    "            torch.save(agent.network.state_dict(), config['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "network = ActorCriticNetwork()\n",
    "agent = ActorCriticAgent(network)\n",
    "\n",
    "config={\n",
    "    \"agent\": \"ActorCriticAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"MC\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_batch\": 2000,\n",
    "    \"episode_per_batch\": 10,\n",
    "    \"output_dir\": \"./models/actor_critic_MC_gamma_0.90\"\n",
    "}\n",
    "\n",
    "agent_run(env, agent, config=config, gamma=0.9)\n",
    "\n",
    "\n",
    "network = ActorCriticNetwork()\n",
    "agent = ActorCriticAgent(network)\n",
    "agent.network.load_state_dict(torch.load(config[\"output_dir\"]))\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/actor_critic_MC_gamma_0.90\", greedy=True)\n",
    "env.reset(seed=seed)\n",
    "demo_run(env, agent, seed=seed, record_video=True, video_folder = f\"./video/actor_critic_MC_gamma_0.90\", greedy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import trainer\n",
    "from agents import (\n",
    "    PolicyGradientNetwork,\n",
    "    PolicyGradientAgent\n",
    ") \n",
    "\n",
    "input_size, hidden_sizes, output_size = 8, [10, 10], 4\n",
    "network = PolicyGradientNetwork(input_size, hidden_sizes, output_size)\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "training_config={\n",
    "    \"agent\": \"PolicyGradientAgent\",\n",
    "    \"phase\": \"train\",\n",
    "    \"project\": \"lunalander\",\n",
    "    \"reward\": \"discounted_future_reward\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"gamma\": 0.99,\n",
    "    \"num_batch\": 500,\n",
    "    \"episode_per_batch\": 5,\n",
    "    \"model_path\": \"./models\",\n",
    "    \"info\": \"network 8x10x10x4\"\n",
    "}\n",
    "\n",
    "def discounted_future_reward(seq_rewards, rewards, gamma):\n",
    "    n = len(seq_rewards)\n",
    "\n",
    "    for i in range(2, n+1):\n",
    "        seq_rewards[-i] += gamma * (seq_rewards[-i+1])\n",
    "    rewards[-len(seq_rewards):] = seq_rewards\n",
    "    return rewards\n",
    "\n",
    "luna_lander_trainer = trainer(training_config, seed=543)\n",
    "luna_lander_trainer.train(discounted_future_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
